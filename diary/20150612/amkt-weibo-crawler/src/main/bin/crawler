#!/bin/bash
# resolve links - $0 may be a softlink
THIS="$0"
# copy mr config file

print_usage () 
{
  echo "Usage: run/crawler COMMAND"
  echo "where COMMAND is one of the follows:"
  echo "	ducrawl	[-threadNum <threadNum>]																抓取服务(监测任务并执行)"
  echo "    sinaBasicCrawler -type <local|hbase> [-s startRowkey] [-e endRowkey] [-i localInputPath]        执行sina基本爬虫，抓取用户详情、微博、tags、关注数据"
  echo "	sinaWebCrawler -type <local|hbase> [-s startRowkey] [-e endRowkey] [-i localInputPath]			执行sina.cn爬虫，抓取职业等信息"
  echo "	sinaTagsCrawler	-type <local|hbase> [-s startRowkey] [-e endRowkey] [-i localInputPath]			批量抓取新浪tags信息"
  echo "    tencentBasicCrawler -type <local|hbase> [-s startRowkey] [-e endRowkey] [-i localInputPath]        执行tencent基本爬虫，抓取用户详情、微博、tags、关注数据"
  echo "	token -src [sn/tx] -i filePath				    											导入爬虫账号"
  echo "    sinaFansCrawler -i <uid>                	                    							抓取新浪userId对应的粉丝数据"
  echo "	sinaFriendsIds -type <local|hbase> [-s startRowkey] [-e endRowkey] [-i localInputPath]		    抓取新浪关注用户Id列表"
  echo "	sinaFollowersCrawler -i <uid> [-dropTmpTbl] [-dropOldFans]		    						通过粉丝关系图深度抓取新浪用户Id的粉丝列表"
  echo "	sinaFansDetailUpdate  -i <uid>															抓取指定用户的所有粉丝的详情"
  echo "    sinaReplyCrawler  -url <url> [-o <outputfile>|-t <hbase table name>] [-f <yeezhao|other> ]	抓取指定微博的评论和转发,f为可选参数，不指定默认为抓取所有用户的"
  echo "	infoSupplement  [-s <startRowkey>] [-e <endRowkey>]			                                                                          补全yeezhao.user.info表中的crawl:tweetsNum等字段"
  echo "	getWeiboText -i <weiboid> -src <src>		                                                                                                                     获取weibo内容，直接打印至屏幕"
  echo "	sinaInteractCrawler -i <uid> [-type <getMention|getWeibo|getCommRepost|all>] [-d day] ]-row <wbRowKey>] 	抓取corp互动数据并保存到yeezhao.corp.interaction表"
  echo "	sinaCrawlMention -d <days> -token <userToken>   										抓取@指定用户的微薄"
  echo "	keywordSearch -i <uid> -src <src> [-rt <runTimes>]   									抓取@指定用户的关键字搜索出来的微薄"
  echo "	sinaGettoken -u <username> -p <password> -key <appkey> -sec <appsecret> 				获取指定app的token"
  echo "	sinaFriends -nickname <nickname> -o <outfile>  											抓取新浪制定nickname所有关注的用户"
  echo "    sinaSendbytoken -i <weibo内容文件> [-img <image file path>] [-rps]						通过api发送微博"
  echo "	sinaGetFnsLst -i <uid> -amnt <fans num>	-o <outputfile>	-id							抓取用户最新的n个粉丝,开启id选项只抓取用户id"
  echo "	sinaUURelationship -i <uid文件> -o <output file> -id <uid>								判断输入文件中的用户是否是查询用户的粉丝"
  echo "	tencentWeiboTimeUpdate -t <tablename> -c meta:,content: [-s startRow] [-e endRow] -o <outputdir> -n regionNum	修正腾讯微博时间"
  echo "    txReplyCrawler  																		抓取腾讯微博的评论和转发"
  echo "    userStat -help																			用户统计"
  echo "	search4Users -key <key word> -size <num of users>	页面搜索新浪用户"
  echo " 	webFetcher											新浪页面抽取"
  echo "	sinaKeywordCrawler									新浪关键字抓取"
  echo "	taobaoTradeCrawler [ -i <tidlist file> | -tid <tid> ] 抓取淘宝用户评价记录,输入淘宝的UID,指定用的所有交易记录保存在哎HBASE"
  echo "	sinaPmsg -help										新浪私信抓取"
  echo "    test -i filePath                             测试"
  echo "    uid2UserInfo -i <localPath> -o<localPath>   	 输出Kol用户群的详细信息"
  echo "    classifyCheckins -i <localPath> -o<localPath>  对Checkins数据进行分类"
  echo "    classifyBooks -i <localPath> -o<localPath>  对Book数据进行分类"
  echo "    classifyMovies -i <localPath> -o<localPath>  对Moive数据进行分类"
  echo "    classifyApps -i <localPath> -o<localPath>  对APP数据进行分类"
  echo "    classifyGoodsAttr -i <localPath> -o<localPath>  对Goods/Attribute数据进行分类"
  echo "    getuid -o filePath                             获取uid，指定存放路径"
  exit 1
}
	
if [ $# = 0 ] || [ $1 = "help" ]; then
  print_usage
fi


# get arguments
COMMAND=$1
shift

# some directories
THIS_DIR=`dirname "$THIS"`
CRAWLER_HOME=`cd "$THIS_DIR/.." ; pwd`

# echo $CRAWLER_HOME;


JAVA=$JAVA_HOME/bin/java
HEAP_OPTS="-Xmx1000m"

CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar
CLASSPATH=${CLASSPATH}:conf
CLASSPATH=${CLASSPATH}:`ls |grep jar|grep bin`
for f in lib/*.jar; do
  CLASSPATH=${CLASSPATH}:$f;
done

# default log directory & file
if [ "$CRAWLER_LOG_DIR" = "" ]; then
  CRAWLER_LOG_DIR="$CRAWLER_HOME/logs"
fi
if [ "$CRAWLER_LOGFILE" = "" ]; then
  CRAWLER_LOGFILE='crawler.log'
fi

if [ -d "$CRAWLER_HOME/target/classes/keywords" ]; then
   rm -rf $CRAWLER_HOME/target/classes/keywords
fi

CRAWLER_OPTS="$CRAWLER_OPTS -Dhadoop.log.dir=$CRAWLER_LOG_DIR"
CRAWLER_OPTS="$CRAWLER_OPTS -Dhadoop/.log.file=$CRAWLER_LOGFILE"

# figure out which class to run
if [ "$COMMAND" = "crawl" ]; then
  CLASS=com.yeezhao.wbcrawler.tools.ChampsParser
fi
# run it
params=$@
java -Dfile.encoding=UTF-8 -Djava.awt.headless=true $HEAP_OPTS $CRAWLER_OPTS -classpath "$CLASSPATH" $CLASS $params
